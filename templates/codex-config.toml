# ~/.codex/config.toml — created by codex-1up
# Adjust to your liking. See Codex CLI docs for full options.

# Core (root defaults)
model = "gpt-5.1-codex"
approval_policy = "on-request"       # untrusted|on-failure|on-request|never
sandbox_mode   = "workspace-write"   # read-only|workspace-write|danger-full-access
profile = "balanced"                 # active profile: balanced|safe|yolo

# Extra settings that only apply when `sandbox = "workspace-write"`.
[sandbox_workspace_write]
# Allow the command being run inside the sandbox to make outbound network
# requests. Disabled by default.
network_access = true

# UI & notifications (can be toggled by installer)
[tui]
# Desktop notifications from the TUI: boolean or filtered list. Default: false
# Examples: true | ["agent-turn-complete", "approval-requested"]
notifications = false
# Show raw reasoning content when available (default: false)
show_raw_agent_reasoning = true
# Ensure we do not hide internal reasoning (default: false)
hide_agent_reasoning = false

# Centralized feature flags — booleans only
[features]
web_search_request = true
unified_exec = false
streamable_shell = false
rmcp_client = false
apply_patch_freeform = false
view_image_tool = true
experimental_sandbox_command_assessment = false
ghost_commit = false
enable_experimental_windows_sandbox = false

# ---- Profiles (override only what differs from root) -----------------------

[profiles.balanced]
approval_policy = "on-request"
sandbox_mode = "workspace-write"
model = "gpt-5.1-codex"
model_reasoning_effort = "medium"
model_reasoning_summary = "concise"
[profiles.balanced.features]
web_search_request = true

[profiles.safe]
approval_policy = "on-failure"
sandbox_mode = "read-only"
model = "gpt-5.1-codex"
model_reasoning_effort = "medium"
model_reasoning_summary = "concise"
[profiles.safe.features]
web_search_request = false

[profiles.yolo]
approval_policy = "never"
sandbox_mode = "danger-full-access"
model = "gpt-5.1-codex-max"
model_reasoning_effort = "high"
model_reasoning_summary = "detailed"
tool_output_token_limit = 25000
model_auto_compact_token_limit = 233000  # 273k - (25k tool_output + 15k buffer)
[profiles.yolo.features]
web_search_request = true
